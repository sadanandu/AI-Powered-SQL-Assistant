# ğŸ’¡ AI-Powered Oracle 23ai Query Assistant (POC)

This project is a proof-of-concept (POC) system that allows users to ask natural language questions and receive answers powered by Oracle Autonomous Database (ADBS) using both SQL and vector search. It integrates a locally running LLM via Ollama and a backend FastAPI-based MCP server that handles execution logic and communication with the database.

The system uses a modular design, enabling intelligent routing of queries, SQL generation, and response interpretation â€” all handled locally.

---

![Alt text](system_design.png)
![Alt text](example.png)


## ğŸ§± System Components

### ğŸ‘¤ User
- Inputs natural language questions.
- Receives final human-readable answers.

### ğŸ§  LLM on Ollama (LLaMA 3.2)
- Interprets user queries and generates appropriate SQL (traditional or semantic).
- Also interprets SQL responses if needed for better human readability.
- Runs locally via [Ollama](https://ollama.com).

### ğŸ¤– SQL Assistant
- Central orchestrator of the entire workflow.
- Routes queries to the LLM, forwards generated SQL to the MCP server, and interprets final results.
- Also performs intermediate logic for formatting or refining output before presenting to the user.

### ğŸŒ MCP Server (FastAPI)
- Receives SQL statements from the SQL Assistant.
- Executes SQL on Oracle ADBS and returns results.
- Acts as the executor, not the decision-maker.

### ğŸ—„ï¸ Oracle Autonomous Database (ADBS)
- Hosts structured and unstructured data.
- Performs both standard SQL and vector-based semantic queries.
- Embedding generation and storage is handled natively using Oracle's `ALL_MINILM_L12_V2` model.

---

## ğŸ” Data Flow Overview

1. **User submits a natural language query** to the SQL Assistant.
2. **SQL Assistant forwards the query** to the local LLM on Ollama.
3. **LLM generates a SQL query** (traditional or vector-based).
4. **SQL Assistant sends the SQL** to the MCP server for execution.
5. **MCP server executes the SQL** on Oracle ADBS and retrieves results.
6. **Results are returned to the SQL Assistant** by the MCP server.
7. **SQL Assistant may optionally send results to LLM** for interpretation.
8. **LLM interprets and explains results** (optional).
9. **Final human-readable output is shown to the user**.

This flow ensures both local processing and flexibility in adapting queries intelligently.

---

## ğŸ”§ Technologies Used

- **Oracle Autonomous Database (23ai)**
- **Oracle Vector Search**
- **LLaMA 3.2 (via Ollama)**
- **FastAPI (MCP Server)**
- **Python + oracledb driver**

---

## ğŸš€ Highlights

- ğŸ§  Fully local LLM with intelligent SQL generation.
- ğŸ—ƒ No external vector DB needed â€” all vector embedding and querying is native to Oracle ADBS.
- âš¡ Smart switching between structured SQL and semantic vector search.
- ğŸ§© Modular architecture for easy expansion (e.g., adding UI, confidence scoring, etc.).
- ğŸ”„ Optional interpretation of SQL results via LLM for more natural answers.

---

## ğŸ“Œ Status

âœ… Embeddings inserted  
âœ… Local LLM integrated via Ollama  
âœ… MCP server connected to Oracle ADBS  
âœ… Semantic and SQL querying tested  
âœ… Diagram-based architecture established  

### ğŸ§ª Next Steps
- UI development for better usability.
- Add confidence scoring and query classification.
- Explore multi-modal inputs (e.g., PDFs, images).
- Implement user feedback loop for query refinement.
